{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping data from product webpages into text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libaries\n",
    "from urllib.request import urlopen, Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libaries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libaries\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#used selenium and chrome to automate a web browser to scroll through webpage since it couldn't be scraped by normal means\n",
    "\n",
    "#append links to each product on the website to list 'Wow'\n",
    "wow = []\n",
    "\n",
    "driver = webdriver.Chrome('C:\\\\Users\\\\akomandu\\\\Python_Stuff\\\\chromedriver.exe')\n",
    "driver.get(\"https://living-future.org/declare/\")\n",
    "driver.maximize_window()\n",
    "\n",
    "SCROLL_PAUSE_TIME = 1.0\n",
    "\n",
    "# Get scroll height\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while True:\n",
    "    # Scroll down to bottom\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    page = driver.page_source\n",
    "    soup = BeautifulSoup(page, 'html.parser')\n",
    "    wow.append(soup.find_all('a', href=True))\n",
    "    # Wait to load page\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "    # Calculate new scroll height and compare with last scroll height\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        break\n",
    "    last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Links = []\n",
    "\n",
    "#filter out extraneous links\n",
    "for i in wow[-1]:\n",
    "    if 'lf-reveal-card' in str(i):\n",
    "        Links.append(i['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#for every link, gather product information and write it to text file\n",
    "for link in Links:\n",
    "    print(link)\n",
    "    page = requests.get(link)\n",
    "    time.sleep(5)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    company = soup.find('strong').get_text()\n",
    "    extra = []\n",
    "    for i in soup.findAll(\"div\", {\"class\": \"declare-option\"}):\n",
    "        extra.append(i.get_text().split('\\n'))\n",
    "    \n",
    "    with urlopen(Request(link, headers={'User-Agent': 'Mozilla'})) as url:\n",
    "        s=url.read()\n",
    "        dfs = pd.read_html(s)\n",
    "    \n",
    "    f = open(link.split('/')[-2] + '.txt', 'w', encoding='utf-8')\n",
    "    j=0\n",
    "    f.write(company)\n",
    "    for i in extra:\n",
    "        i[1] = i[1] + ':'\n",
    "        f.write(' '.join(i) + '\\n')\n",
    "    f.write('\\n')\n",
    "    f.write(' '.join(list(dfs[0])) + '\\n')\n",
    "    for index,row in dfs[0].iterrows():\n",
    "        if j > len(dfs):\n",
    "            break\n",
    "        else:\n",
    "            f.write(str(row[0]) + '__' + str(row[1]) + '__' + str(row[2]) + '__' + str(row[3]) + '__' + str(row[4]))\n",
    "            f.close()\n",
    "            j+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Text Files to PDF Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os     ## Allows operating system commands to be performed though Python\n",
    "import sys    ## Allows you to exit a Python script for testing\n",
    "import shutil ## Allows movement of files from one directory to another\n",
    "import  glob  ## Global searching of strings as files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\akomandu\\\\Python_Portfolio\\\\PDF_Scrapes\\\\Living_Future\\\\Files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#gather text files\n",
    "files = glob.glob('C:\\\\Users\\\\akomandu\\\\Python_Portfolio\\\\PDF_Scrapes\\\\Living_Future\\\\Files\\\\*.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import libraries for conversion\n",
    "import reportlab\n",
    "from textwrap import wrap\n",
    "from reportlab.lib.pagesizes import A4\n",
    "from reportlab.pdfgen.canvas import Canvas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#switch to new folder to put PDFs in\n",
    "os.chdir('C:\\\\Users\\\\akomandu\\\\Python_Portfolio\\\\PDF_Scrapes\\\\Living_Future\\\\PDFs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create PDFs\n",
    "for i in files:\n",
    "    f = open(i, encoding='utf-8')\n",
    "    title = str(i.split('\\\\')[-1][:-3] + 'pdf')\n",
    "    text = f.read()\n",
    "    okay = text.split('\\n')\n",
    "    for n,i in enumerate(okay):\n",
    "        if 'Description:' in i:\n",
    "            okay[n] = '\\n'.join(wrap(i, 100))\n",
    "    wrapped = '\\n'.join(okay)\n",
    "    print(title)\n",
    "    c = Canvas(title, pagesize=A4)\n",
    "    t = c.beginText()\n",
    "    t.setTextOrigin(50, 800)\n",
    "    t.textLines(wrapped)\n",
    "    c.drawText(t)\n",
    "    c.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
